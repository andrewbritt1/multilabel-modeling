# Unified Configuration for All Training and Inference Modes
# ===========================================================
# This configuration file supports all modes:
# Training:
#   - multiclass: Single-label classification training
#   - multilabel: Multi-label classification training
#   - mlm: Masked Language Model pretraining
# Inference:
#   - multiclass-inference: Single-label classification inference
#   - multilabel-inference: Multi-label classification inference
#
# Usage Examples:
#   python unified_model.py --mode multiclass -c unified_config.yaml
#   python unified_model.py --mode multilabel -c unified_config.yaml
#   python unified_model.py --mode mlm -c unified_config.yaml
#   python unified_model.py --mode multiclass-inference -c unified_config.yaml
#   python unified_model.py --mode multilabel-inference -c unified_config.yaml
# ===========================================================

# ============================================================================
# Model Configuration (Training Only)
# ============================================================================
model:
  name: "bert-base-uncased"  # HuggingFace model for training
  # Alternative models for different use cases:
  # name: "distilbert-base-uncased"  # 40% smaller, 60% faster
  # name: "roberta-base"  # Often better performance than BERT
  # name: "albert-base-v2"  # Memory efficient with parameter sharing
  # name: "microsoft/deberta-v3-base"  # State-of-the-art performance
  # name: "google/electra-base-discriminator"  # Efficient pretraining
  # name: "bert-base-multilingual-cased"  # For multilingual tasks
  use_torch_compile: false  # Enable torch.compile optimization (PyTorch 2.0+)

# ============================================================================
# Data Configuration (Training Only)
# ============================================================================
data:
  # Path to training data CSV file
  # Expected formats by mode:
  # - multiclass: columns [text_column, label_column]
  # - multilabel: columns [text_column, label1, label2, ...]
  # - mlm: column [text_column]
  path: "data/train.csv"
  
  # Column containing text data
  text_column: "text"
  
  # For multiclass mode only - column containing single label
  label_column: "category"
  
  # Data splitting configuration
  test_size: 0.2  # Validation split for classification modes
  random_state: 42  # Random seed for reproducible splits
  
  # Sequence length configuration
  max_length: 512  # Maximum tokens (or "auto" for MLM to auto-calculate)
  length_percentile: 95  # Percentile for auto-calculating max_length

# ============================================================================
# Training Configuration
# ============================================================================
training:
  # Batch size (adjust based on GPU memory and mode)
  batch_size: 32  # Good default for most cases
  # Recommended batch sizes:
  # - multiclass: 32-64 (depending on sequence length)
  # - multilabel: 16-32 (more memory intensive)
  # - mlm: 16-32 (depending on model size)
  
  # Number of training epochs
  epochs: 10  # Good for classification tasks
  # Recommended epochs:
  # - multiclass: 5-15 (depending on dataset size)
  # - multilabel: 10-20 (may need more for convergence)
  # - mlm: 1-5 (for domain adaptation)
  
  # Learning rate (typically 2e-5 to 5e-5 for BERT-like models)
  learning_rate: 5e-5
  
  # Validation split (used if test_size not specified)
  val_split: 0.2
  
  # Performance optimizations
  fp16: true  # Mixed precision training (faster, less memory)
  gradient_checkpointing: true  # Trade compute for memory
  num_workers: 0  # DataLoader workers (0 avoids multiprocessing issues)
  
  # Directory for checkpoints during training
  output_dir: "output"

# ============================================================================
# MLM-Specific Configuration (MLM Mode Only)
# ============================================================================
mlm:
  mlm_probability: 0.15  # Probability of masking each token (standard is 15%)
  do_whole_word_mask: true  # Mask whole words instead of subword tokens
  train_test_split: 0.05  # Smaller validation set for MLM (5% is typical)

# ============================================================================
# Inference Configuration (Inference Modes Only)
# ============================================================================
inference:
  # Input/output files for inference
  input_file: "data/test.csv"  # CSV file with text to classify
  output_file: "predictions.csv"  # Where to save predictions
  
  # Path to trained model directory (from training output)
  model_path: "models/bert-base-uncased-multiclass-20240101_120000"
  # model_path: "models/bert-base-uncased-multilabel-20240101_120000"
  
  # Column in input file containing text
  text_column: "text"
  
  # Threshold for positive predictions (multilabel only)
  threshold: 0.5  # Predictions >= threshold are positive
  
  # Inference settings
  batch_size: 32  # Can be larger than training since no gradients
  max_length: 512  # Should match training max_length
  device: "cuda"  # Device to use: "cuda" or "cpu"
  
  # Optional: Override model's id2label mapping
  # Useful if you need custom label names or order
  # id2label:
  #   '0': 'negative'
  #   '1': 'neutral'
  #   '2': 'positive'
  
  # For multilabel:
  # id2label:
  #   '0': 'technical'
  #   '1': 'urgent'
  #   '2': 'bug'
  #   '3': 'feature_request'

# ============================================================================
# Tokenizer Configuration (Optional)
# ============================================================================
tokenizer:
  max_length: null  # Override max length for tokenization
  length_percentile: 95  # Override percentile calculation

# ============================================================================
# MLflow Experiment Tracking (Training Only)
# ============================================================================
mlflow:
  enabled: true  # Enable/disable MLflow tracking
  experiment_name: "unified-transformer-experiments"  # MLflow experiment name
  tracking_uri: null  # MLflow server URI (null = local server)
  autostart: true  # Auto-start local MLflow server if not running
  port: 5000  # Port for local MLflow server
  backend_store_uri: "./mlruns"  # Where to store MLflow data
  artifact_store: null  # Artifact storage location (null = default)

# ============================================================================
# Logging Configuration
# ============================================================================
logging:
  level: "INFO"  # Log level: DEBUG, INFO, WARNING, ERROR, CRITICAL
  file: null  # Log file path (null = console only)
  # file: "logs/training.log"  # Enable file logging

# ============================================================================
# Output Configuration (Training Only)
# ============================================================================
output:
  dir: "models"  # Directory to save final trained models

# ============================================================================
# Miscellaneous
# ============================================================================
# Random seed for reproducibility (training only)
seed: 42

# ============================================================================
# Example Configurations by Use Case
# ============================================================================

# --- Sentiment Analysis (Multiclass) ---
# model:
#   name: "distilbert-base-uncased"
# data:
#   path: "data/sentiment_train.csv"
#   text_column: "review"
#   label_column: "sentiment"
# training:
#   batch_size: 64
#   epochs: 10
#   learning_rate: 3e-5

# --- Document Tagging (Multilabel) ---
# model:
#   name: "roberta-base"
# data:
#   path: "data/documents_train.csv"
#   text_column: "content"
# training:
#   batch_size: 16
#   epochs: 15
#   learning_rate: 2e-5

# --- Domain Adaptation (MLM) ---
# model:
#   name: "bert-base-uncased"
# data:
#   path: "data/domain_corpus.csv"
#   text_column: "text"
#   max_length: "auto"
# training:
#   batch_size: 32
#   epochs: 3
#   learning_rate: 5e-5
# mlm:
#   mlm_probability: 0.15
#   do_whole_word_mask: true

# --- Inference Configuration ---
# inference:
#   input_file: "data/new_reviews.csv"
#   output_file: "sentiment_predictions.csv"
#   model_path: "models/distilbert-sentiment-20240101"
#   text_column: "review"
#   batch_size: 64
#   device: "cuda"

# ============================================================================
# Performance Tuning Guide
# ============================================================================

# For Limited GPU Memory:
# -----------------------
# - Reduce batch_size (try 8 or 16)
# - Enable gradient_checkpointing: true
# - Use fp16: true
# - Consider using distilbert instead of bert
# - Reduce max_length if possible

# For Faster Training:
# --------------------
# - Increase batch_size if memory allows
# - Use distilbert or albert models
# - Enable fp16: true
# - Set num_workers: 4 (if not causing issues)

# For Better Accuracy:
# --------------------
# - Use larger models (roberta-large, deberta-v3-large)
# - Increase epochs
# - Try lower learning rates (2e-5 or 3e-5)
# - Increase max_length if texts are long
# - For multilabel: tune the threshold parameter

# For Production Inference:
# -------------------------
# - Use distilbert for 2x faster inference
# - Increase inference batch_size
# - Consider quantization or ONNX conversion
# - Use CPU if latency is not critical